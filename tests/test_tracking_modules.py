#!/usr/bin/env python3
import os
import sys



import numpy as np

from needle import Tensor

# ðŸ”§ Adjust this to your actual file path
# e.g. from vggt_needle.blocks.attn_blocks import Mlp, AttnBlock, CrossAttnBlock
from vggt_needle.heads.track_modules.modules import Mlp, AttnBlock, CrossAttnBlock
from needle import backend_ndarray as nd
device = nd.cuda() if nd.cuda().enabled() else nd.cpu()
print(device)

def test_mlp_linear():
    """
    Test Mlp in pure linear mode (use_conv=False).
    Input: (B, T, C)
    """
    print("Testing Mlp (linear mode)...")
    B, T, C = 2, 5, 16
    hidden = 32
    x_np = np.random.randn(B, T, C).astype("float32")
    x = Tensor(x_np).to(device)

    mlp = Mlp(
        in_features=C,
        hidden_features=hidden,
        out_features=C,
        use_conv=False,
        drop=0.0,
    ).to(device)

    y = mlp(x)
    y_np = y.numpy()
    assert np.isfinite(y_np).all()
    assert y_np.shape == x_np.shape, f"Mlp (linear) shape mismatch: {y_np.shape} vs {x_np.shape}"
    print("  âœ… Mlp (linear) forward OK; output shape:", y_np.shape)


def test_mlp_conv():
    """
    Test Mlp in conv mode (use_conv=True).
    Here Mlp uses 1x1 convolutions, so we feed NCHW.
    """
    print("Testing Mlp (conv mode)...")
    N, C, H, W = 2, 16, 8, 8
    hidden = 32
    x_np = np.random.randn(N, C, H, W).astype("float32")
    x = Tensor(x_np).to(device)

    mlp = Mlp(
        in_features=C,
        hidden_features=hidden,
        out_features=C,
        use_conv=True,
        drop=0.0,
    ).to(device)

    y = mlp(x)
    y_np = y.numpy()
    assert np.isfinite(y_np).all()
    assert y_np.shape == x_np.shape, f"Mlp (conv) shape mismatch: {y_np.shape} vs {x_np.shape}"
    print("  âœ… Mlp (conv) forward OK; output shape:", y_np.shape)


def test_attn_block():
    """
    Test AttnBlock forward.
    Input: (B, T, hidden_size)
    """
    print("Testing AttnBlock...")
    B, T, hidden_size, num_heads = 2, 6, 32, 4
    x_np = np.random.randn(B, T, hidden_size).astype("float32")
    x = Tensor(x_np).to(device)

    block = AttnBlock(
        hidden_size=hidden_size,
        num_heads=num_heads,
    ).to(device)

    y = block(x)  # mask=None
    y_np = y.numpy()
    assert np.isfinite(y_np).all()
    assert y_np.shape == x_np.shape, f"AttnBlock output shape mismatch: {y_np.shape} vs {x_np.shape}"
    print("  âœ… AttnBlock forward OK; output shape:", y_np.shape)


def test_cross_attn_block():
    """
    Test CrossAttnBlock forward.
    - x: (B, T_q, hidden_size)
    - context: (B, T_k, hidden_size)
    """
    print("Testing CrossAttnBlock...")
    B = 2
    T_q = 5
    T_k = 7
    hidden_size = 32
    num_heads = 4

    x_np = np.random.randn(B, T_q, hidden_size).astype("float32")
    ctx_np = np.random.randn(B, T_k, hidden_size).astype("float32")

    x = Tensor(x_np).to(device)
    context = Tensor(ctx_np).to(device)

    block = CrossAttnBlock(
        hidden_size=hidden_size,
        context_dim=hidden_size,
        num_heads=num_heads,
    ).to(device)

    # mask=None; CrossAttnBlock will call cross_attn(x, context, context, attn_mask=None)
    y = block(x, context, mask=None)
    y_np = y.numpy()
    assert np.isfinite(y_np).all()
    assert y_np.shape == x_np.shape, f"CrossAttnBlock output shape mismatch: {y_np.shape} vs {x_np.shape}"
    print("  âœ… CrossAttnBlock forward OK; output shape:", y_np.shape)


def main():
    np.random.seed(0)
    test_mlp_linear()
    test_mlp_conv()
    test_attn_block()
    test_cross_attn_block()
    print("\nAll block forward tests passed âœ…")


if __name__ == "__main__":
    main()
